---
layout: post
title: MLE / MAP
category_num: 6
---

# MLE / MAP

- Christopher M. Bishop의 Pattern Recognition and Machine Learning을 참고하여 작성했습니다.
- update date : 2020.03.04

## 최적의 파라미터 $$\mu$$ 찾기

끝 부분이 휘어져 앞면/뒷면이 나올 확률이 각각 0.5가 아닌 동전이 있다고 하자. 이러한 동전을 던졌을 때 앞면이 나올 확률을 구하고 싶다면 어떻게 해야 할까. 통계학에서는 빈도주의와 베이지안 두 가지 방법이 있는데, 빈도주의에서 사용하는 방법은 다음과 같다.

### 1. Frequentism: MLE

최적의 파라미터를 찾기 위해 빈도주의 통계학에서는 **Maximum Likelihood Estimation(MLE)**을 사용한다. 즉 Likelihood를 극대화하는 파라미터 $$\mu$$를 찾겠다는 것이다. likelihood 라는 것이 어떤 파라미터를 가정했을 때 주어진 사건이 발생할 확률을 의미하므로, 이것이 극대화되면 해당 확률을 가장 잘 표현하는 파라미터를 찾아냈다고 할 수 있다.

데이터셋 $$D = {x_1, x_2, ... x_n}$$이 있다고 하자. 첫 번째 시행에서 앞면이 나왔다면 $$x_1$$은 1이고, 두 번째 시행에서 뒷면이 나왔다면 $$x_2$$는 0인 식이다. 그리고 각각의 시행이 **i.i.d condition**을 만족한다고 하면, 다음과 같은 likelihood 식을 생각해 볼 수 있다.

$$
\eqalign{
p(D \lvert \mu)
&= \Pi_{k=1}^N p(x_k \lvert \mu)\\
& = \Pi_{k=1}^N \mu^{x_k} (1 - \mu)^{1 - x_k}
}
$$

계산의 편의를 위해 log를 씌우면 다음과 같이 전개된다.

$$
log p(D \lvert \mu) = \Sigma_{k = 1}^N x_k log \mu + (1 - x_k) log (1-\mu)
$$

우리의 목표는 $$log p(D \lvert \mu)$$를 극대화하는 파라미터 $$\mu$$를 찾는 것이므로, 이를 $$\mu$$에 대해 미분하여 0이 되는 $$\mu$$를 구하면 다음과 같아진다.

$$\mu_{MLE} = {1 \over N} \Sigma_{k=1}^N x_k$$

하지만 빈도주의의 MLE는 **over fitting**의 경향이 매우 심하게 나타난다는 문제가 있다. 어떤 동전을 세 번 던졌을 때 세 번 모두 앞면이 나왔다고 가정해보자. 이 경우

$$\mu_{MLE} = {1 \over N} \Sigma_{k=1}^N x_k = {3 \over 3} = 1$$

이 성립한다. 즉, 해당 동전을 던질 경우 항상 앞면이 나온다고 보는 것이다. 다소 극단적인 결론인데, 조금 더 뒤에서 언급할 베이지안에서는 prior 함수를 가정한 채 posterior를 극대화하는 방법으로 접근하여 이러한 문제에서 자유로워질 수 있다.

### 2. Beysianism: MAP

빈도주의와 베이지안의 가장 큰 차이 중 하나는 파라미터에 대한 관점이다. 위에서 보았듯이 빈도주의에서는 파라미터를 어떤 특정한 수 $$\mu$$로 본다. 반면 베이지안에서는 파라미터 $$\mu$$를 확률변수로 보고, 확률 분포 $$p(\mu)$$로 다룬다.

- 빈도주의의 파라미터: $$\mu$$, 정해진 어떤 수
- 베이지안의 파라미터: $$p(\mu)$$, 확률 변수

베이지안에서는 **prior**를 가정하고 아래와 같은 베이지안 규칙에 따라 posterior를 극대화하는 방법으로 문제를 해결한다. 이러한 점에서 **Maximum A Posterior(MAP)**라고 한다.

$$
posterior \varpropto likelihood \ \cdot \ prior
$$

베이지안 접근법으로 문제를 해결하기 위해서는 적절한 prior를 가정해야 한다. 이와 관련하여 다음과 같은 사항을 고려할 필요가 있다.

- prior와 posterior는 모두 확률 분포이다. 따라서 다루기 쉬운 확률분포로 가정하는 것이 좋다.
- prior는 likelihood와 곱해진다. 따라서 likelihood와 곱해지기 쉬운 형태여야 한다.
- 그리고 곱해진 결과는 posterior와 유사해야 한다.

위의 내용들을 고려하면 결과적으로 likelihood와 곱해지기 쉬운 형태로, prior와 posterior가 동일한 확률 분포여야 한다. 이러한 특성을 conjugate라고 한다. **베타분포**는 likelihood가 이항분포인 경우 prior로 자주 사용되는 확률 분포이다.

## Beta distribution

베타 분포는 이항분포의 모수를 적절히 추정하기 위해 prior로 사용되기에 적절한 확률분포라고 할 수 있다. 베이지안의 관점에서 이야기하면 어떤 사건의 발생 여부에 대한 믿음의 크기를 확률 분포의 형태로 표현한 것이다. 베타 분포의 형태는 다음과 같다.

$$
Beta(\mu \lvert a, b) = {\Gamma(a + b) \over \Gamma(a)\Gamma(b)} \mu^{a-1} (1 - \mu)^{b-1}
$$

$$\Gamma(x)$$는 gamma function으로, 다음과 같이 정의된다.

$$
\Gamma(x) = \int_0^\infty u^{x-1}e^{-u}du
$$

베타 분포는 다음과 같은 특성을 갖는다.

- 0에서 1사이의 확률 분포: $$\int_0^1 Beta(\mu \lvert a, b) d\mu = 1$$
- mean: $$E[\mu] = {a \over a+b}$$
- variance: $$var[\mu] = {ab \over (a + b)^2 (a+b+1)}$$

### Beta distribution as prior

Prior로 Beta 분포를, likelihood로 이항분포를 가정하자.

- prior: $$ Beta(\mu \lvert a, b) = {\Gamma(a + b) \over \Gamma(a)\Gamma(b)} \mu^{a-1} (1 - \mu)^{b-1} $$
- likelilhood: $$\begin{pmatrix} N \\ m \end{pmatrix} \mu^m (1 - \mu)^{N - m}$$

이때 $$l = N - m$$으로 하면 다음과 같이 prior, likelihood와 posterior 간의 관계를 나타낼 수 있다.

- posterior: $$
p(\mu \lvert m, l, a, b) \varpropto \mu^{m+a-1} (1 - \mu)^{l+b-1}
$$

정확한 posterior는 다음과 같다.

- posterior: $$
p(\mu \lvert m, l, a, b) = {\Gamma(m+a+l+b) \over \Gamma(m+a) \Gamma(l+b)} \mu^{m+a-1} (1 - \mu)^{l+b-1}
$$

여기서 $$m$$은 $$x=1$$인 경우의 수, $$l$$은 반대로 $$x=0$$인 경우의 수를 뜻한다. 그리고 베타함수의 $$a$$와 $$b$$는 prior에서 각 사건 $$x=1$$, $$x=0$$에 대한 파라미터이다. 이를 effective number of observations 라고도 한다.

### Sequential approach

결과적으로 보면 prior와 posterior가 모두 베타 분포가 된다. 따라서 기존의 prior와 데이터로 구한 posterior를 다음 예측의 prior로 사용하는 것도 가능하다. 극단적으로는 데이터셋에 들어있는 개별 $$x_k$$를 한 번에 하나씩 posterior를 업데이트하고, 다음 prior로 사용하는 방식으로 개선하는 것 또한 가능하다. 이를 위해서는 i.i.d condition을 만족해야 하고, 한 번 사용한 데이터를 다시 사용해서는 안 된다.

우리의 목표가 $$x = 1$$인지 아닌지에 대한 확률 $$p(x=1)$$을 구하는 것이라고 하자. 데이터셋 $$D$$를 관찰한 후 $$p(x=1)$$은 $$p(x \lvert D)$$가 되며, 이는 다음과 같이 전개가 가능하다.

$$
\eqalign{
p(x = 1 \lvert D) &= \int_0^1 p(x = 1 \lvert \mu) p(\mu \lvert D) d\mu \\
&= \int_0^1 \mu p(\mu \lvert D) d\mu = E[\mu \lvert D]
}
$$

위에서 베타 분포의 특성 중 베타 분포의 평균은 $$E[\mu] = {a \over a + b}$$라고 보았다. 그리고 $$a, b$$가 각각 사전에 $$x=1$$, $$x=0$$를 관찰한 갯수에 대응함을 고려하면, $$E[\mu \lvert D]$$는 다음과 같이 표현할 수 있다.

$$
p(x = 1 \lvert D) = E[\mu \lvert D] = {m + a \over m + a + l + b}
$$

쉽게 생각해보면, 기존에는 $$p(x = 1) = {a \over a + b}$$ 정도라고 생각했는데 $$x = 1$$을 $$m$$개, $$x = 0$$을 $$l$$개 포함하고 있는 데이터셋 $$D$$을 보고나니 $$p(x = 1 \lvert D) = {m + a \over m + a + l + b}$$가 더 맞다고 고쳐 생각하게 되었다는 것이다.

## Relation of MLE and MAP

여기서 재미있는 점은 무한히 많은 데이터를 보는 경우, 즉 데이터셋 $$D$$의 크기가 무한히 커서 $$m \rightarrow \infty$$, $$l \rightarrow \infty$$인 경우라면 베이지안 방식과 MLE의 방식이 동일해진다는 것이다. 그리고 데이터셋이 유한한 경우에는 posterior를 통해 얻은 파라미터가 prior로 얻은 파라미터와 MLE의 방식으로 추정한 파라미터 사이에 위치한다는 점이다. 즉 데이터를 많이 보면 많이 볼수록 posterior의 파라미터는 MLE 방식의 파라미터에 보다 가까워진다.
