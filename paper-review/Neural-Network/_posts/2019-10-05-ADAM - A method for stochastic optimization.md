---
layout: post
title: ADAM) A method for stochastic optimization
---

# ë…¼ë¬¸ ì œëª© : ADAM) A method for stochastic optimization

- Kingma ë“±
- 2014
- [ë…¼ë¬¸ ë§í¬](<https://arxiv.org/abs/1412.6980>)
- 2019.10.05 ì •ë¦¬

## ì„¸ ì¤„ ìš”ì•½

- ADAMì€ íš¨ê³¼ì ì´ê³  ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” Optimization ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ Adagradì™€ RMSProp ê³¼ ê´€ë ¨ì´ ê¹Šë‹¤.
- RMSPropê³¼ ê°™ì´ ê°€ì¤‘í‰ê· ì„ ì´ìš©í•˜ì—¬ step sizeë¥¼ ê²°ì •í•œë‹¤. ì´ë¥¼ í†µí•´ í•™ìŠµì˜ ì •ë„ì— ë”°ë¼ step sizeë¥¼ ìë™ì ìœ¼ë¡œ ì¡°ì ˆ(automatic anealing)í•  ìˆ˜ ìˆë‹¤.
- ì´ì— RMSPropê³¼ëŠ” ë‹¬ë¦¬ ê°€ì¤‘í‰ê· ì˜ ê²°ê³¼ë¡œ ìƒê¸°ëŠ” biasë¥¼ ì œê±°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ í¬í•¨ë˜ì–´ ìˆë‹¤.

## ë‚´ìš© ì •ë¦¬

### ADAMì´ë€

- stochastic optimization ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì´ê¸°ë„ í•˜ë‹¤.
- "Adam is an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments"
- first-order optimization method, ì¦‰ 1ì°¨ ë¯¸ë¶„ê°’ë§Œì„ ì‚¬ìš©í•œë‹¤.
  - high-order optimization methodëŠ” ë¶€ì í•©í•˜ë‹¤ê³  ì§€ì í•œë‹¤.
- ë˜í•œ ì ì€ memory ì‚¬ìš©ì´ ì¥ì ì´ë‹¤.
- ADAMì€ ê¸°ë³¸ì ìœ¼ë¡œ AdaGradê³¼ RMSPropì˜ ì˜í–¥ì„ ë°›ì•˜ë‹¤.
  - AdaGradëŠ” sparse gradientì—ì„œ, RMSPropëŠ” on-line, non-stationary settingì—ì„œ ì˜ ì‘ë™í•˜ëŠ” ë°©ë²•ì´ë¼ê³  í•œë‹¤.

### AdaGrad

- adagradì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

  $$
  \eqalign{
  &g_t = \nabla(J(\theta))\\

  &G_t = G_{t-1} + (g_t)^2\\

  &\theta_t = \theta_{t-1} - \alpha ( {n \over \root 2 \of {G_t + \epsilon}}) g_t
  }
  $$

- ì—¬ê¸°ì„œ $$g_t$$ëŠ” objective functionì˜ gradientì´ë‹¤.
- ìˆ˜ì‹ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ ëˆ„ì ëœ ê³¼ê±° gradientì˜ ë³€í™”ëŸ‰($$G_t$$)ì´ í¬ë©´ í´ìˆ˜ë¡ í˜„ì¬ $$g_t$$ ì˜ ì˜í–¥ë ¥ì´ ì‘ì•„ì§„ë‹¤.
- AdaGradëŠ” iterationì´ ì»¤ì§€ë©´ step-sizeê°€ ê³¼ë„í•˜ê²Œ ì‘ì•„ì§„ë‹¤ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆë‹¤.
- ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì œí”„ë¦¬ í•¸íŠ¼ ë“±ì€ **RMSProp**ë¥¼ ì œì•ˆí–ˆë‹¤.

### RMSProp

- RMSPropì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

  $$
  \eqalign{
  &G_t = \gamma G_{t-1} + (1-\gamma)(g_t)^2\\
  &\theta_t = \theta_{t-1} - \alpha ({ n \over \root \of G_t + \epsilon })g_t
  }
  $$

- AdaGramì™€ ë¹„êµí•´ ë³¼ ë•Œ $$G_t$$ë¥¼ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì´ ë‹¬ë¼ì¡Œë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¨ìˆœíˆ ë”í•˜ë˜ ê²ƒì´ ê°€ì¤‘í‰ê· (WMA, Weighted Moving Average)ìœ¼ë¡œ ë°”ë€Œì—ˆë‹¤.
- ì´ë¥¼ í†µí•´ $$G_t$$ê°€ ì»¤ì§€ëŠ” ì†ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.
- ë‹¤ë§Œ RMSPropëŠ” ê°€ì¤‘í‰ê· ì„ ì ìš©í•˜ë©´ì„œ biasê°€ ìƒê¸´ë‹¤ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆë‹¤. ì¦‰, í•™ìŠµ ì´ˆê¸°ì— WMAëŠ” 0ë¶€í„° ì‹œì‘í•˜ê²Œ ë˜ë¯€ë¡œ ì‹¤ì œ gradientì˜ ë¶„í¬ì™€ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” gradientì˜ ë¶„í¬ì— ì°¨ì´ê°€ ìƒê¸´ë‹¤ëŠ” ê²ƒì´ë‹¤.
  - ì´ë¡œ ì¸í•´ ì¸í•´ ìˆ˜ë ´ì´ ì˜ ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤ê³  í•œë‹¤.
- ADAMì€ RMSPropì˜ biasë¥¼ ì œê±°í•˜ëŠ” ë°ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤.

### ADAMì˜ Algorithm

- ADAMì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
- ì´ë•Œ $$\beta_1$$ ë¡œëŠ” 0.9, $$\beta_2$$ ë¡œëŠ” 0.999 ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤.

$$
\eqalign{
  &m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1)g_t \\
  &v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2)g_t^2
}
$$

- first moment $$m$$ê³¼ second moment $$v$$ ë¥¼ ì‚¬ìš©í•œë‹¤.

$$
\hat m_t = {m_t \over 1 - \beta_1^t } \qquad \hat v_t = {v_t \over 1 - \beta_2^t}
$$

- biasë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ìœ„ì™€ ê°™ì´ ë‚˜ëˆ„ê¸°ë¥¼ ì‹¤ì‹œí•œë‹¤.

$$
\theta_t = \theta_{t-1} - \alpha ({\hat m_t \over \root \of {\hat v_t + \epsilon}})
$$

- RMSPropì—ì„œ Gğ— ë¥¼ vğ—hatìœ¼ë¡œ, $$g_t$$ ë¥¼ $$m_t$$ ë¡œ ë°”ê¾¼ ê²ƒê³¼ ë™ì¼í•˜ë‹¤.

#### ADAMì˜ upate rule

- $$\epsilon$$ì´ 0 ì´ë¼ê³  ê°€ì •í•œë‹¤ë©´ $$\alpha({m_t \over \root \of {\hat v_t}})$$ ë§Œí¼ $$\theta$$ê°€ ë³€í™”í•˜ê²Œ ëœë‹¤(step).
- $$t$$ ì‹œì ì—ì„œì˜ stepì„ $$\Delta t$$ ë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ì„±ë¦½í•œë‹¤.

$$
\eqalign{
&if \ (1 - \beta+1) > \root \of {1 - \beta_2}\\
&\qquad then \ \Delta t \leqq \alpha({1 - \beta_1 \over \root \of {1 - \beta_2}})\\
&elif \ (1 - \beta+1) < \root \of {1 - \beta_2}\\
&\qquad then \ \Delta t \leqq \alpha\\
&elif \ (1 - \beta+1) = \root \of {1 - \beta_2}\\
&\qquad then \ \Delta t < \alpha \quad ... \quad (\because \lvert {\hat m_t \over \root \of {\hat v_t} } \lvert < 1)
}
$$

- ë”°ë¼ì„œ $$\lvert \Delta t \rvert$$ ì˜ í¬ê¸°ëŠ” $$\alpha$$ ë³´ë‹¤ ì‘ê±°ë‚˜ í¬ê²Œ ë„˜ì§€ ì•ŠëŠ” ìˆ˜ì¤€ì—ì„œ ë¹„ìŠ·í•˜ë‹¤($$\lvert \Delta t \rvert \lessapprox \alpha$$).
- ì´ë¥¼ $$\lvert \Delta t \rvert$$ ì˜ trust region ì´ë¼ê³  í•œë‹¤.
- ê·¸ë¦¬ê³  $$\lvert {\hat m_t \over \root \of {\hat v_t} } \lvert$$ ì´ í•™ìŠµì˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ê¸°ì¤€ì´ ë˜ë©° ì´ë¥¼ **SNR**(signal-to-noise ratio)ë¼ê³  í•œë‹¤.
  - SNRì˜ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì‘ì„ìˆ˜ë¡ $$\lvert \Delta t \rvert$$ì´ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.
  - optimaì— ê°€ê¹Œìš¸ìˆ˜ë¡ SNRì˜ í¬ê¸°ê°€ ì‘ë‹¤. ì´ëŸ¬í•œ ì ì—ì„œ **automatic annealing**ì´ë¼ê³  í•œë‹¤.

#### bias correction

- ë”¥ëŸ¬ë‹ì—ì„œ í•™ìŠµì„ í•  ë•Œì—ëŠ” gradientë¥¼ ê¸°ì¤€ìœ¼ë¡œ weight $$\theta$$ ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
- ê·¸ëŸ°ë° RMSPropì™€ ê°™ì´ ê°€ì¤‘í‰ê· ì„ ì´ìš©í•˜ê²Œ ë˜ë©´ gradientë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì•„ biasê°€ ë°œìƒí•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ê°€ì¤‘í‰ê· ì˜ ì²« ë²ˆì§¸ ê°’ì´ 0ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œ gradientì˜ ë¶„í¬ì™€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê°’ì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§„ë‹¤.
  - ADAM ë˜í•œ ê°€ì¤‘í‰ê· ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•œë‹¤.
  - ADAMì˜ ì•Œê³ ë¦¬ì¦˜ì„ ë³´ë©´ $$g_t$$ ë¥¼ $$\hat v_t$$ ìœ¼ë¡œ ëŒ€ì‹ í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  $$\hat v_t$$ ì€ $$v_t$$ì—ì„œ $$(1-\beta_2^t)$$ ë¥¼ ë‚˜ëˆˆ ê°’ì´ë‹¤.
  - ì¦‰ $$v_t$$ì˜ ë¶„í¬ë¥¼ $$g_t$$ ì— ë§ì¶”ê¸° ìœ„í•´, ê·¸ë¦¬ê³  biasë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ $$1-\beta_2^t$ ë¥¼ ë‚˜ëˆˆ ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. êµ¬ì²´ì ì¸ ì´ìœ ëŠ” ì•„ë˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

#####  $$1-\beta_2^t$$ ë¥¼ ë‚˜ëˆ„ì–´ ì£¼ëŠ” ì´ìœ 

$$
\eqalign{
&v_t = ( 1 - \beta_2) \Sigma^t_{i=1} \beta_2^{t-i} \cdot g_i^2 \\
& \eqalign{E[v_t] &= [(1 - \beta_2 \Sigma_{i=1}^t) \beta_2^{t-i}\cdot g_i^2]\\
    &= E[g_t^2] (1-\beta_2) \Sigma_{i=1}^t \beta_2^{t-i} + \zeta \\
    &= E[g_t^2] (1-\beta_2) + \zeta
  }
}
$$

- ì¦‰ $$v_t$$ ì˜ ë¶„í¬ì™€ $$g_t^2$$ ì— $$1 - \beta_2^t$$ ë¥¼ ê³±í•´ì¤€ ë¶„í¬ê°€ ë¹„ìŠ·í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
