---
layout: post
title: Wolpertinger Architecture) Deep Reinforcement Learning in Large Discrete Action Spaces
category_num: 10
---

# 논문 제목 : Deep Reinforcement Learning in Large Discrete Action Spaces

- Gabriel Dulac-Arnold 등
- 2016
- <https://arxiv.org/abs/1512.07679>
- 2019.10.08 정리

## Summary

- action space가 클 때에는 기존의 방법들로는 학습이 잘 되지 않는다. 이러한 문제를 해결하기 위해 actor-critic을 기반으로 한 Wolpertinger Architecture를 제시한다.
- Wolpertinger Architecture란 continuous action을 결정하는 algorithms을 categorical action을 결정하는 데에 사용하는 방법이다.
- PG로 얻은 continuous action vector(proto-action)를 기준으로 KNN을 통해 유사한 valid action들로 구성된 집합을 만든다. 그 중 Q-value가 가장 큰 action을 선택한다.

## Problem: large action space

- 발전된 AI system은 매 step마다 많은 수의 action 중에 하나를 골라야 하는 문제를 가지고 있다.
  - Youtube, Amazon 등에서 사용되는 추천시스템, 살제 산업 환경에서 사용되는 로봇의 통제 시스템 등이 대표적이다.
- 이러한 경우 지금까지 나온 방식으로는 학습에 어려움이 많았다. 특히 neural net을 이용하면 연산량이 action의 수의 증가에 따라 매우 크게 늘어나고, 학습도 잘 되지 않는다.
  - "This quickly becomes intractable, especially if the parameterized function is costly to evaluate, as is the case with deep neural networks"
- 이러한 문제를 해결하기 위해서 논문에서는 "Wolpertinger Architecture" 방식을 제시한다.

## Comparison with other algorithms

### value-based architecture

- action의 수가 많아지면 연산량이 크게 늘어나고 학습이 어려워진다는 문제점이 있다.
- 하지만 어떤 action a에 대한 학습이 진행되면 인접한 action $$a'$$에 대한 학습도 어느 정도 이뤄진다는 장점이 있다.
  - 이 경우 보지 못한 action들에 대해서도 어느 정도 학습된 결과를 보일 수 있다.
- 문제는 action의 수가 너무 많으면 이러한 장점도 기대하기 어려워진다는 점이다.

### actor-based architecture

- value-based 에 비해 연산량이 작다.
- 하지만 과거에 보지 못한 action에 대해서는 전혀 학습이 이뤄지지 않는다.
  - "actor-based approaches do not generalize over the action space as naturally as value-based approaches, and cannot extend to previously unseen actions"

## Wolpertinger Architecture

- 위에 제시된 두 가지 접근법의 장점은 취하고 단점은 줄이는 방법이라고 논문에서 제안하는 알고리즘이다.
- 기본적으로 actor-critic 구조를 가지고 있다.
- actor와 critic 모두 각각 독립적인 neural net을 가지고 있다.

### Actor of Wolpertinger Architecture

- actor는 기본적으로 state를 input으로 받고 action을 output으로 내보낸다. 즉, $$\pi_\theta(a \lvert s)$$라고 할 수 있다.
- Wolpertinger Architecture에서는 action a를 정하는데에 다음과 같은 algorithms을 사용한다.
    1. Environment로 부터 state s를 받는다.
    2. $$a'$$을 $$f_\theta(s)$$로 부터 구한다.
        - 여기서 $$a'$$를 proto-action이라고 한다.
    3. $$A_k$$ = g_k(a')을 구한다.
        - 여기서 $$A_k$$는 선택 가능한 action의 집합 A의 부분집합으로, KNN을 이용해 $$a'$$와 가까운 순서대로 복수의 action을 포함하고 있다.
    4. $$A_k$$ 중 $$Q$$ value의 값($$Q_\theta'(s, a)$$)이 가장 큰 action a를 구한다.
    5. action a를 environment에 input으로 하고, reward r과 다음 state s'를 구한다.
- Actor에서 가장 중요한 것은 $$f_\theta(\cdot)$$의 역할이다.
  - $$f_\theta(\cdot)$$는 PG 알고리즘을 따른다. 따라서 $$f_\theta(\cdot)$$의 output은 discrete action이 아니라 continuous action이 된다. 이렇게 구해진 vector를 proto-action $$a'$$이라고 한다.
    - 정확하게 $$f_\theta(\cdot)$$의 반환값은 ℝⁿ 이다. 이때 $$n$$은 action의 차원의 수이다.
  - 하지만 여기서 문제는 $$a'$$으로는 $$Q$$ value를 바로 구할 수 없다는 것이다. 왜냐하면 proto-action이 선택 가능한 action의 집합 A에 포함되어 있지 않을 수 있기 때문이다.
    - 논문에서는 이를 not valid action이라고 표현한다.
- 이를 해결하기 위해 도입한 것이 $$g_k(a')$$ 이다. $$g_k(a')$$는 선택 가능한 action의 집합 $$A$$에서 $$a'$$과 가장 유사한 action을 KNN으로 구해 이들로 구성된 부분집합을 반환한다.
  - 그렇다면 왜 특정한 action이 아닌 action의 집합 $$A_k$$ 를 반환하는 것일까. 논문에서는 $$a'$$과 가장 가까운 action이 최선의 action은 아닐 수 있다고 주장한다.
    1. 주변 action의 q-value가 전반적으로 높지만 가장 가까운 action의 value는 매우 낮을 수 있다.
    2. 가장 가까운 action의 q-value가 주변 action들에 비해 특별히 낮은 long-term value를 가지고 있을 수도 있다.
  - 즉 무조건 가장 가까운 action을 고르는 것으로는 $$a'$$의 가치를 제대로 판단할 수 없고, 가까운 여러 개의 action 중 q-value가 가장 높은 action의 q-value를 $$a'$$의 가치로 보자는 것이다.
- 최종적으로 actor는 $$\pi_\theta(s) = \arg \max_{a \in g_k \cdot f_\theta\pi(s)} Q_{\theta^Q}(s,a) $$ 의 식으로 표현된다.

### Critic of Wolpertinger Architecture

- critic 부분은 기본적인 actor-critic과 크게 다르지 않다.
- 다만 critic 의 value function QΘ를 업데이트 할 때에 replay buffer를 사용한다.

### updating parameters of actor and critic

- "The critic is trained from samples stored in a replay buffer. Actions stored in the replay buffer are generated by $$\pi_\theta(s)$$, but the policy gradient $$\nabla_a Q_\theta (s,a)$$ is taken at $$a'=f_\theta(s)$$"
- "The target action in the Q-update is generated by the full policy $$\pi_\theta$$ and not by $$f_\theta$$"
- actor의 parameter $$\theta$$ 는 $$Q_\theta(s,a)$$ 값을 통해 구해진 TD-error를 이용해 업데이트하게 된다. $$f_\theta()$$의 출력값은 $$a'$$이지만 $$a'$$이 $$a$$로 대표되기 때문에 $$a$$의 TD-error를 기준으로 업데이트 하는 것이다.
- critic의 parameter $$\theta$$는 $$(s,a,r,s')$$의 집합으로 구성되어 있는 memory buffer를 이용해 업데이트 한다. 그 방법은 일반적인 DQN과 같다.
