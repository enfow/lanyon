---
layout: post
title: Dueling DQN) Dueling Network Architectures for Deep Reinforcement Learning
---

# ë…¼ë¬¸ ì œëª© : Dueling Network Architectures for Deep Reinforcement Learning

- Ziyu Wang ë“±
- 2015
- <https://arxiv.org/abs/1511.06581>
- 2019.09.21 ì •ë¦¬

## ì„¸ ì¤„ ìš”ì•½

- Dueling architectureë€ ëª¨ë“  actionì— ëŒ€í•´ ì§ì ‘ì ìœ¼ë¡œ Q valueë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ value function Vì™€ advantage function Aì˜ í•©ìœ¼ë¡œ Q valueë¥¼ êµ¬í•˜ëŠ” êµ¬ì¡°ë¥¼ ë§í•œë‹¤.
- ì´ì™€ ê°™ì´ ë‘ ê°œë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ Advantage updating ì´ë¼ê³  í•˜ë©° ì´ë¥¼ í†µí•´ ë¹ ë¥´ê³  ì •í™•í•œ ìˆ˜ë ´ì´ ê°€ëŠ¥í•˜ë‹¤.
- ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ Prioritized replayë¥¼ ì‚¬ìš©í–ˆë‹¤.

## ë‚´ìš© ì •ë¦¬

### Deuling architecture

- ë§ì€ stateì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ê²½ìš°ì˜ actionì„ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ë‹¤.
  - ì˜ˆë¥¼ ë“¤ì–´ Enduro game ì—ì„œëŠ” ì¶©ëŒ ì§ì „ì—ë§Œ ì™¼ìª½ ë˜ëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì›€ì§ì´ë©´ ëœë‹¤. í•˜ì§€ë§Œ ë§ì€ stateì—ì„œëŠ” ì–´ë– í•œ actionì„ ì·¨í•˜ë˜ ê°„ì— ì¶©ëŒì´ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.
  - ë°˜ë©´ ì–´ë– í•œ í–‰ë™ì„ ì„ íƒí•˜ë”ë¼ë„ ì¶©ëŒì´ ë°œìƒí•˜ëŠ” ê²½ìš°ë„ ì¡´ì¬í•œë‹¤.
- ì´ëŸ¬í•œ ì ì„ ê³ ë ¤í•´ ë³¼ ë•Œ ëª¨ë“  stateì—ì„œ ëª¨ë“  actionì˜ valueë¥¼ êµ¬í•˜ê³  ê·¸ì— ë”°ë¼ í–‰ë™í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ì¢‹ì§€ ì•Šì€ stateì— ì²˜í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤.
- ì´ëŸ¬í•œ ì ì—ì„œ ê° stateì˜ value functionì„ ê°ì•ˆí•˜ì—¬ actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

### Deuling architectureì˜ êµ¬ì¡°ì  íŠ¹ì„±

- Dueling architectureë€ state value representationê³¼ action advantage representationì˜ í•©ìœ¼ë¡œ Q-functionì˜ ê°’ì„ êµ¬í•˜ëŠ” êµ¬ì¡°ë¥¼ ë§í•œë‹¤.
  - "explicitly seperated the represenation of state values and (state-dependent) action adcantage"
- ë‘ ê°€ì§€ represenationì€ í•˜ë‚˜ì˜ CNN moduleì„ ê³µìœ í•œë‹¤.
- CNN module ì´í›„ ë‚˜ëˆ„ì–´ì§„ ë‘ ê°œì˜ represenationì€ íŠ¹ì •í•œ ì§€ì •(supervision)ì—†ì´ ìë™ì ìœ¼ë¡œ state value functionê³¼ advantage functionì˜ ì¶”ì •ì¹˜ë¥¼ ë§Œë“¤ê²Œëœë‹¤. ê·¸ë¦¬ê³  íŠ¹ìˆ˜í•œ ê²°í•© ë ˆì´ì–´(special aggregating layer)ì— ì˜í•´ í•©ì³ì ¸ state-action value Q-functionì„ ë§Œë“¤ê²Œ ëœë‹¤.

- V functionì˜ ê°’ì€ scalar ê°’ì´ ë˜ê³ , A functionì˜ ê°’ì€ 'í˜„ì¬ stateì—ì„œ ì·¨í•  ìˆ˜ ìˆëŠ” ëª¨ë“  actionë“¤ì˜ ìˆ˜'ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” vectorê°’ì´ ëœë‹¤. ê·¸ë¦¬ê³  ìµœì¢… Q functionì˜ ê°’ì€ vectorì˜ ê° elementì— scalar ê°’ì„ ë”í•˜ì—¬ êµ¬í•˜ê²Œ ëœë‹¤.
  - `Q(s, a; Î¸, Î±, Î²) = V (s; Î¸, Î²) + A(s, a; Î¸, Î±)`
- ì—¬ê¸°ì„œ ë¬¸ì œ ì¤‘ í•˜ë‚˜ëŠ” ë”í•´ì§„ Që¡œëŠ” Vì™€ Aì˜ ê°’ì„ recover í•˜ê¸° ì–´ë µë‹¤ëŠ” ì ì´ë‹¤.
  - "unidentifiable in the sense athe given Q we cannot recover V and A uniquely"
  - ì´ëŠ” ì ì ˆí•œ í•™ìŠµì„ ë°©í•´í•˜ëŠ” ìš”ì¸ì´ ë˜ë©° ê²°ê³¼ì ìœ¼ë¡œ performanceì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤.
- ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ A ì— max(A)ì˜ ê°’ì„ ë¹¼ì£¼ì–´ Aì˜ ëª¨ë“  elementê°€ 0 ë˜ëŠ” ìŒìˆ˜ì˜ ê°’ì„ ê°€ì§€ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.
  - `Q(s, a; Î¸, Î±, Î²) = V (s; Î¸, Î²) + (A(s,a;Î¸,Î±)âˆ’ max A(s,aâ€²;Î¸,Î±))`
  - `Q(s, a*; Î¸, Î±, Î²) = V (s; Î¸, Î²)` ê°€ ì„±ë¦½í•˜ê²Œ ë˜ì–´ V ê°’ì„ íŒë‹¨í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
- ì•ˆì •ì ì¸ optimizationì„ ìœ„í•´ max Aì˜ ê°’ì´ ì•„ë‹Œ Aì˜ í‰ê· ê°’ì„ ë¹¼ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì‚¬ìš©í–ˆë‹¤.

### Advantage updating

- value functionê³¼ advantage function ë‘ ê°œë¡œ ë‚˜ëˆ„ëŠ” ì•„ì´ë””ì–´ëŠ” 1993ë…„ Bairdì— ì˜í•´ ì²˜ìŒ ì‚¬ìš©ë˜ì—ˆë‹¤.
  - ì¼ë°˜ì ì¸ Q-learningë³´ë‹¤ ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ê°€ íŠ¹ì§•ì´ë‹¤.
- Advantage functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
  - `Qğ…(s, a) = Eğ’”'[r + ğ›„ maxğ–º' Q*(s',a')| s, a]`
  - `Ağ…(s, a) = Qğ…(s, a) - Vğ…(s)`
- ì¦‰ Advantage function A(s, a) ëŠ” Q function ê°’ì— V function ê°’ì„ ëº€ ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì ì—ì„œ A(s, a)ëŠ” ê° actionì˜ ìƒëŒ€ì ì¸ ê°€ì¹˜ë¥¼ ë‚˜íƒ€ë‚¸ëŒ€ê³  í•  ìˆ˜ ìˆë‹¤.
  - "The advantage function subtracts the state value from Q to obtain a relative measure of the importance of each action"
- `Eğ–ºï½ğ…(ğ‘ )[Ağ…(s,a)] = 0` ì´ ì„±ë¦½í•œë‹¤.

### Prioritized replay

- "The key idea was to increase the replay probability of experience tuples that have a high expected learning process"
- priority replayë¥¼ í†µí•´ DDQN ë“± ì—¬ëŸ¬ RL ê¸°ë²•ë“¤ì´ SOTAë¥¼ ì°ì—ˆë‹¤.
- <https://arxiv.org/abs/1511.05952>

## reference

- <https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/>
