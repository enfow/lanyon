---
layout: post
title: "Pre-training of Deep Bidirectional Transformers for Language Understanding"
category_num : 1
keyword: '[BERT]'
---

# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## Summary

- 
- 
- 

## Model Architecture

BERT는 **Bidirectional Encoder Representations from Transformers** 의 약자로, 이름에서 알 수 있듯이  Bidirectional 한 특성을 가지는 Transformer의 Encoder 구조를 적극적으로 활용한다. 실제로 논문의 저자들은 자신들이 BERT 의 구현에 사용한 코드가 Transformer 구현을 기초로 하였으며, 거의 동일하기 때문에 모델 구조에 대한 언급은 생략한다고 밝히고 있다.

```
BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation. ... Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture
```

<img src="{{site.image_url}}/paper-review/transformer-model-architecture.png" style="width: 100%; margin: auto; display: block">


보다 구체적으로 BERT 는 Transformer 의 Encoder 구조에서 도입된 Bidirectional self-attention 메커니즘을 적극적으로 사용한다. 직역하면 '양방향' 정도가 되는 Bidirectional 은 무엇을 의미할까. 이는 입력 Sequence 를 처리하는 방식과 관련있다.

단어 그 자체로 의미를 가지기도 하지만 대부분의 자연어는 문장과 같은 단어들의 Sequence 로 의미 전달이 이뤄진다. 어떤 문장이 주어져 있을 때, 사람이 이를 읽고 이해하는 방법은 왼쪽에서부터 오른쪽으로 읽는 방법(left-to-right) 하나와 오른쪽에서 왼쪽으로 읽는 방법(right-to-left) 두 가지가 있다. 그리고 이러한 방법들을 한 방향으로 읽는다고 하여 Unidirectional 이라고 한다.

### Unidirectional Model: GPT-1, ELMo

#### GPT-1

대표적인 Unidirectional Model 로는 **GPT-1(Generative Pre-trained Transformer)** 이 있다. GPT-1 또한  Transformer 구조에서 영감을 받아 만들어진 모델인데, BERT 와 차이가 있다면 Transformer 의 Encoder 가 아닌 Decoder 를 활용한다는 점이다.

<img src="{{site.image_url}}/paper-review/gpt-1-architecture.png" style="width: 30%; margin: auto; display: block">

위 이미지는 GPT-1 논문에서 보여주고 있는 GPT-1 의 구조이다. 첫 번째 Transformer 의 Decoder 와 비교해 볼 때 전체적인 구조가 비슷하나, Encoder 의 출력 값을 사용하여 Attention Score 를 구하는 과정(Cross Self Attention)이 생략되었음을 알 수 있다. GPT-1 은 Transformer 의 Encoder 처럼 이전 step의 output 을 입력 받으며 한 번에 한 단어씩 순차적으로 추론하게 된다. 이러한 점에서 GPT-1 은 Unidirectional Model 이라고 하는 것이다.

#### ELMo

ELMo 는 GPT-1 처럼 left-to-right 방향으로 학습할 뿐만 아니라 right-to-left 방향으로도 학습을 진행하고, 두 방향에서 얻은 embedding 들을 모두를 사용한다. 하지만 BERT 논문의 저자들은 `Shallow concatnation independently trained left-to-right and right-to-left LMs.` 이라고 표현하며 BERT의 `Deep bidirectional representation` 과는 차이가 있다고 언급한다.

### Bidirectional Model

그렇다면 BERT 에서 말하는 Bidirectional 의 의미는 무엇일까. 이름에서도 알 수 있듯이 Transformer 의 Encoder 연산을 보면 된다. Transformer 의 Encoder 는 token sequence 를 입력으로 받아, 각각에 맞는 feature 들을 출력한다. Decoder 는 이들 feature 를 받아 Attention 에 적용하여 최종 예측 값을 생성하게 된다. 즉 BERT 에서 말하는 Bidirectional 은 어느 한 방향으로 순차적으로 정보를 읽어들이는 것이 아닌, 전체 sequence 를 한 번에 입력으로 받고 그들 정보를 모두 활용하여 추론하는 것을 의미한다.

<img src="{{site.image_url}}/paper-review/bert-diff-gpt-elmo-arch.png" style="width: 30%; margin: auto; display: block">


## Training Strategy
