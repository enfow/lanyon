---
layout: post
title: Test
category_num : 1
keyword: '[논문 리뷰]'
---

http://localhost:4000/draft/2021/09/04/test/


# Continuous Unconstrainted Optimization

## Problem Statement

$$
\min_{x \in \mathcal{R}^n} f(x) \qquad f: \mathcal{R}^n \rightarrow \mathcal{R}
$$

### Iterative Method

$$
x_{k+1} = x_k + d
$$

### Gradient Descent

$$
x_{k + 1} = x_k - \alpha \nabla f(x)
$$

### Newton's Method

$$
x_{k + 1} = x_k - \nabla^2 f(x)^{-1} \nabla f(x)
$$

### Quasi Newton's Method

$$
x_{k + 1} = x_k - \alpha H_k \nabla f(x)
$$


### Quasi Newton's Method Algorithm

$$
\eqalign{
&\text{Init} \ x_0, H_0 \\
&\text{For} \ k = 0, 1, 2, ... \\
& \qquad \eqalign{
    &d_k = - H_k \nabla f(x) \\
    &\text{Get step size: } \alpha_k = \arg \min_{\alpha \geq 0} f(x_k + \alpha d_k) \\
    &\text{Update x: } x_{k + 1} = x_k + \alpha_k d_k \\
    &\text{Update H: } H_k \rightarrow H_{k+1}
}\\
&\text{End For}
}
$$

## Modified Newton's Method

$$
\text{search direction: } \ d_k = - \nabla^2 f(x_k)^{-1} \nabla f(x_k) = x_{k+1} - x_k
$$

$$
\text{there exist an } \bar \alpha > 0, \forall \alpha \in (0, \bar \alpha) \ f(x_k + \alpha d_k) < f(x_k)
$$

$$
\eqalign{
    & \quad \ \phi(\alpha) = f(x_k + \alpha d_k)\\
    &\rightarrow \phi'(\alpha) = \nabla f(x_k + \alpha d_k) d_k\\
    &\eqalign{\rightarrow \phi'(0)
        &= \nabla f(x_k) d_k\\ 
        &= - \nabla f(x_k) \nabla^2 f(x_k)^{-1} \nabla f(x_k) < 0
    }\\
}
$$

$$
\because \nabla f(x_k) \ne 0, \nabla^2 f(x_k)^{-1} > 0
$$

$$
\phi'(0) < 0
$$

$$
\text{so there exist } \bar \alpha, \ \text{satisfies} \ \phi(\alpha) < \phi (0) \ \forall \alpha \in (0, \bar \alpha) 
$$

$$
\Rightarrow \alpha_k = \arg \min_{\alpha \geq 0} f(x_k - \alpha \nabla^2 f(x_k)^{-1} \nabla f(x_k))
$$

$$
H > 0
$$

$$
H(x_{k+1} - x_k) = \nabla f(x_{k+1}) - \nabla f(x_k)
$$

## Newton Direction

$$
f(x_k + p) \approx f(x_k) + p^T \nabla f(x_k) + {1 \over 2} p^T \nabla^2 f(x_k)p
$$

$$
\eqalign{
&\rightarrow \nabla f(x_k) + {1 \over 2} p^T(\nabla^2 f(x_k)^T + \nabla^2 f(x_k)) = 0\\
&\rightarrow \nabla f(x_k) + p^T \nabla^2 f(x_k) = 0 \qquad ( \because \text{Hessian is symmetric}) \\ 
&\rightarrow p = - \nabla f(x_k)^{-1} \nabla f(x_k) \quad \qquad (\text{Newton Direction}) \\ 
}
$$

## Taylor 근사와 Hessian

$$
\eqalign{
f(x) &\approx f(a) + \nabla f(a) (x - a) + {1 \over 2} \nabla^2 f(a) (x - a)^2 + ... \\

\nabla f(x_{k+1}) &\approx \nabla f(x_k) + \nabla^2 f(x^k) (x_{k+1} - x_k)
}
$$

$$
\nabla^2 f(x_k) = { \nabla f(x_{k+1}) - \nabla f(x_k) \over x_{k+1} - x_k}
$$

$$
\nabla^2 f(x_k) = { \nabla f(x_{k+1}) - \nabla f(x_k) \over x_{k+1} - x_k}
$$

$$
\nabla^2 f(x_k) (x_{k+1} - x_k) = \nabla f(x_{k+1}) - \nabla f(x_k)
$$

$$
\nabla^2 f(x_k)^{-1} (\nabla f(x_{k+1}) - \nabla f(x_k)) = x_{k+1} - x_k
$$

$$
H (\nabla f(x_{k+1}) - \nabla f(x_k)) = x_{k+1} - x_k
$$

## Single Rank Symmetric algorithm

$$
H_{k+1} = H_k + \alpha_k z_k z_k^T
$$

$$
\alpha \in \mathcal{R}, z \in \mathcal{R^n}
$$

$$
z z^T = 

\begin{bmatrix}
a \\ b \\ c
\end{bmatrix}

\begin{bmatrix}
a & b & c
\end{bmatrix}

=

\begin{bmatrix}
aa & ab & ac \\
ba & bb & bc \\
ca & cb & cc
\end{bmatrix}
$$

$$
 g_k = \nabla f(x_k)
$$

$$
\Delta g_k = \nabla f(x_{k+1}) - \nabla f(x_k)
$$

$$
\Delta x_k = x_{k+1} - x_k
$$

$$
H_{k+1} \Delta g_k = \Delta x_k
$$

$$
\eqalign{
& H_{k+1} \Delta g_k = (H_k + \alpha_k z_k z_k^T) \Delta g_k \\
\rightarrow & \Delta x_k = (H_k + \alpha_k z_k z_k^T) \Delta g_k \\
\rightarrow & \Delta x_k - H_k \Delta g_k = (\alpha_k z_k^T \Delta g_k) z_k \\
\rightarrow & z_k = { \Delta x_k - H_k \Delta g_k \over \alpha_k z_k^T \Delta g_k }
}
$$

$$
\eqalign{
& z_k = { \Delta x_k - H_k \Delta g_k \over \alpha_k z_k^T \Delta g_k } \\
\rightarrow & \alpha_k z_k z_k^T = { (\Delta x_k - H_k \Delta g_k) (\Delta x_k - H_k \Delta g_k)^T \over \alpha_k (z_k^T \Delta g_k)^2 } \\
\rightarrow & H_{k+1} = H_k + { (\Delta x_k - H_k \Delta g_k) (\Delta x_k - H_k \Delta g_k)^T \over \alpha_k (z_k^T \Delta g_k)^2 } \\
\rightarrow & H_{k+1} = H_k + { (\Delta x_k - H_k \Delta g_k) (\Delta x_k - H_k \Delta g_k)^T \over \Delta g_k^T (\Delta x_k - H_k \Delta g_k)} \\
}
$$

$$
\because  \alpha_k (z_k^T \Delta g_k)^2  = \Delta g_k^T (\Delta x_k - H_k \Delta g_k)
$$

$$
\Delta g_k^T (\Delta x_k - H_k \Delta g_k)
$$

$$
\because H_{k+1} = H_{k}+ \alpha_k z_k z_k^T
$$

## Davidon-Fletcher-Powell Algorithm

$$
H_{k+1} = H_k + { \Delta x_k \Delta_k^T \over \Delta x_k^T \Delta g_k } - {(H_k \Delta g_k) (H_k \Delta g_k)^T \over \Delta g_k^T H_k \Delta g_k }
$$

$$
\eqalign{
x^T H_{k+1} x &= x^T H_k x + { x^T \Delta x_k \Delta_k^T x \over \Delta x_k^T \Delta g_k } - {x^T (H_k \Delta g_k) (H_k \Delta g_k)^T x \over \Delta g_k^T H_k \Delta g_k } \\
&= x^T H_k x + { (x^T \Delta x_k)^2 \over \Delta x_k^T \Delta g_k } - { (x^T H_k \Delta g_k)^2 \over \Delta g_k^T H_k \Delta g_k } \\
}
$$

$$
\eqalign{
& x^T H_k x + { (x^T \Delta x_k)^2 \over \Delta x_k^T \Delta g_k } - { (x^T H_k \Delta g_k)^2 \over \Delta g_k^T H_k \Delta g_k } \\
= & a^T a + { (x^T \Delta x_k)^2 \over \Delta x_k^T \Delta g_k } - { (a^T b)^2 \over b^T b } \\
= & {a^T a b^T b \over b^T b}  - { (a^T b)^2 \over b^T b } + { (x^T \Delta x_k)^2 \over \Delta x_k^T \Delta g_k } \\
= & {\| a \|^2 \| b \|^2 - (<a, b>)^2 \over \| b \|^2} + {(x^T \Delta x_k)^2 \over \Delta x_k^T \Delta g_k }
}
$$

$$
a = H_k^{1/2} x
$$

$$
b = H_k^{1/2} \Delta g_k
$$

$$
<a, b> = \root \of {a^T b} \quad (\text{inner product})
$$

$$
\text{Proposition: } d_k^T g_{k+1} = 0 \text{ in a conjugated direction algorithm}
$$

$$
\eqalign{
\rightarrow \Delta x_k^T \Delta g_k &= \Delta x_k^T (g_{k+1} - g_k) \\
&= - \alpha_k d_k^T g_{k+1} + \alpha_k \Delta x_k^T g_k \\
&= \alpha_k g_k^T H_k g_k \\
}
$$

$$
\because \Delta x_k = - \alpha_k H_k g_k
$$

$$
\rightarrow {\| a \|^2 \| b \|^2 - (<a, b>)^2 \over \| b \|^2} + {(x^T \Delta x_k)^2 \over \Delta x_k^T \Delta g_k } = {\| a \|^2 \| b \|^2 - (<a, b>)^2 \over \| b \|^2} + {(x^T \Delta x_k)^2 \over \alpha_k g_k^T H_k g_k }
$$

$$
\rightarrow x^T H_{k+1} x = {\| a \|^2 \| b \|^2 - (<a, b>)^2 \over \| b \|^2} + {(x^T \Delta x_k)^2 \over \alpha_k g_k^T H_k g_k } > 0
$$

$$
> 0
$$

$$
\geq 0
$$

$$
\eqalign{
& \root \of {a^T b} \leq \root \of {a^T a} \root \of {b^T b} \\
\rightarrow & a^T b \leq a^T a b^T b \\
\rightarrow & 0 \leq a^T a b^T b - a^T b 
}
$$

$$
g_k^T H_k g_k > 0
$$

$$
\alpha_k > 0
$$

## BFGS Alogrihtm

$$
\eqalign{
    \text{Approx Hessian Inverse: } &H \\
    \text{Approx Hessian: } &B \\
}
$$

$$
\eqalign{
&H_{k+1} \Delta g_k = \Delta x_k\\
\rightarrow &B_{k+1} \Delta x_k = \Delta g_k
}
$$

$$
\eqalign{
\Delta g_k &\rightarrow \Delta x_k\\
\Delta x_k &\rightarrow \Delta g_k\\
H &\rightarrow B
}
$$

$$
B_{k+1} = B_k + { \Delta g_k \Delta g_k^T \over \Delta g_k^T \Delta x_k } - {(B_k \Delta x_k) (B_k \Delta x_k)^T \over \Delta x_k^T B_k \Delta x_k }
$$

$$
H_{k+1}^{\text{BFGS}} = B_{k+1}^{-1} = \left(B_k + { \Delta g_k \Delta g_k^T \over \Delta g_k^T \Delta x_k } - {(B_k \Delta x_k) (B_k \Delta x_k)^T \over \Delta x_k^T B_k \Delta x_k }\right)^{-1}
$$

$$
(A + uv^T)^{-1} = A^{-1} - { (A^{-1}u) (v^T A^{-1}) \over 1 + v^T A^{-1} u}
$$

$$
\eqalign{
    A&: \text{nonsingular} \\
    u, v&: 1 + v^T A u \ne 0 \\
}
$$

$$
H_{k+1}^{\text{BFGS}} = H_k + \left( 1 + { \Delta g_k^T H_k \Delta g_k \over \Delta g_k^T \Delta x_k} \right) { \Delta x_k \Delta x_k^T \over \Delta x_k^T \Delta g_k} - { H_k \Delta g_k \Delta x_k^T + (H_k \Delta g_k \Delta x_k^T)^T \over \Delta g_k^T \Delta x_k}
$$

# Reinforcement Learning

$$
\eqalign{
    V_\pi(s) 
    &= \ E_\pi[G_t \vert s_t = s]\\
    &= \ E_\pi[r_{t} + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... \vert s_t = s]\\
    &= \ E_\pi[\Sigma_{k=0}^\infty \gamma^k r_{t + 1 + k} \vert s_t = s]\\
    &= \ E_\pi[r_t + \gamma \Sigma_{k=0}^\infty \gamma^k r_{t + 2 + k} \vert s_t = s]\\
    &= \ E_\pi[r_t + \gamma G_{t+1} \vert s_t = s]\\
    &= \ E_\pi[r_t + \gamma V_{\pi}(s_{t+1}) \vert s_t = s]\\
}
$$

$$
V_\pi(s_1)
$$

$$
Q_\pi(s_1, a_1)
$$

$$
Q_\pi(s_1, a_2)
$$

$$
\eqalign{
    V_\pi^*(s) &= \max_\pi V(s)\\
    Q_\pi^*(s, a) &= \max_\pi Q(s, a)
}
$$

$$
\eqalign{
V^*(s) = \max_\pi V_\pi(s) \qquad &\forall s \in S\\
Q^*(s, a) = \max_\pi Q_\pi(s, a)  \qquad &\forall (s, a) \in (S, A)
}
$$

$$
\eqalign{
Q_{\pi}(s, a) &= \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma V_\pi(s')]\\
&= \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma \Sigma_{a'} \pi(a' \vert s') Q(s', a') ]\\
}
$$

$$
\eqalign{
Q^*_{\pi}(s, a) &= \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma \Sigma_{a'} \pi^*(a' \vert s') Q^*(s', a') ]\\
&= \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma \max_{a'} Q^*(s', a') ]\\
}
$$

$$
V_\pi(s) = \Sigma_a \pi(a \vert s) Q_\pi(s, a)
$$

$$
Q_\pi (s_1, a_1)
$$

$$
Q_\pi (s_1, a_2)
$$

$$
\eqalign{
    &E_\pi[r_t + \gamma V_{\pi}(s_{t+1}) \vert s_t = s]\\
    &= \Sigma_a \pi(a \vert s) \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma V_\pi(s')]\\
}
$$

### Bellman Equation - State Value Function

$$
V_{\pi}(s) = \Sigma_a \pi(a \vert s) \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma V_\pi(s')]\\
$$

### Bellman Optimality Equation - State Value Function

$$
\eqalign{
V^*_{\pi}(s) &= \Sigma_a \pi^*(a \vert s) \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma V^*_\pi(s')]\\
&= \max_a \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma V^*_\pi(s')]\\
}
$$

$$
V^*_{\pi}(s) = \max_a \Sigma_{s'} p(s' \vert s, a) [r(s, a) + \gamma V^*_\pi(s')]
$$

$$
\eqalign{
V_{\pi}(s_1) &= \Sigma_a \pi(a \vert s_1) \Sigma_{s'} p(s' \vert s_1, a) [r(s_1, a) + \gamma V_\pi(s')]\\
&= \pi(a = \text{left} \vert s_1) [r(s_1, a = \text{left}) + \gamma V_\pi(s')]\\
& \quad + \ \pi(a = \text{right} \vert s_1) [r(s_1, a = \text{right}) + \gamma V_\pi(s')]\\
& \quad + \ \pi(a = \text{down} \vert s_1) [r(s_1, a = \text{down}) + \gamma V_\pi(s')]\\
&= {1 \over 3} [r0 + \gamma 0] + \ {1 \over 3} [0 + \gamma 0] + \ {1 \over 3} [-10 + \gamma 0]\\
&= -{10 \over 3}
}
$$

$$
A = \{ \text{left}, \text{right}, \text{down} \}
$$

$$
P(s + \text{left} \vert s, a=\text{left})\\
P(s + \text{right} \vert s, a=\text{right})\\
P(s + \text{up} \vert s, a=\text{up})\\
P(s + \text{down} \vert s, a=\text{down})\\
$$

$$
\eqalign{
r(s = \text{ Trap }, a) &= - 10 \\
r(s = \text{ Finish }, a) &= +100 \\
r(s = \text{ Else }, a) &= 0 \\
}
$$

$$
\pi(a_1 \vert s_1) 
$$

$$
\pi(a_2 \vert s_1) 
$$

$$
p (s_2 \lvert a_1, s_1) 
$$

$$
p (s_3 \lvert a_1, s_1) 
$$

$$
p (s_3 \lvert a_2, s_1) 
$$

$$
p (s_4 \lvert a_2, s_1) 
$$

$$
r(s_1, a_1) + \gamma V_\pi (s_2)
$$

$$
r(s_1, a_1) + \gamma V_\pi (s_3)
$$

$$
r(s_1, a_2) + \gamma V_\pi (s_3)
$$

$$
r(s_1, a_2) + \gamma V_\pi (s_4)
$$