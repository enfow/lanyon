---
layout: post
title: Decision Theory
category_num: 4
---

# Decision Theory

- Christopher M. Bishop의 Pattern Recognition and Machine Learning을 참고하여 작성했습니다.
- update date : 2020.02.07

## Classification

머신러닝에서 분류 문제란 어떤 주어진 값을 알맞는 클래스로 구분짓는 것이다. 즉 training set에 담겨있는 정보를 통해 분류의 기준을 세워 이후 새로운 정보 $$x$$ 각각에 대해 정확한 클래스를 지정해주는 것이다.

분류 문제의 불확실성은 결합 확률 분포 $$P(X, t)$$로 표현할 수 있다. 예를 들어 $$P(X=x_i, t=1)$$는 어떤 $$x_i$$가 주어졌을 때 이것이 $$t=1$$으로 분류될 확률을 의미한다. 만약 $$x_i$$가 실제로 $$t=1$$이라면 오차가 적을 것이고, 그렇지 않다면 오차는 클 것이다. 이때 training set을 이용하여 $$P(X, t)$$를 결정하는 과정을 **추론**(inference) 과정이라고 한다.

### Diagnosis problem

분류 문제의 대표적인 예시로 어떤 환자의 X-ray 사진을 바탕으로 암에 걸렸는지를 진단하는 문제가 있다. 이때 입력값 $$X$$는 각 환자의 X-ray 사진이 된다. 그리고 target value $$t$$는 0과 1 사이의 숫자로, $$t=1$$이면 암에 걸렸다는 것을 의미하는 클래스 $$C_1$$로, $$t=0$$이면 그렇지 않은 클래스 $$C_2$$로 분류하는 데에 사용된다.

- $$t = 1$$ : present of cancer, class $$C_1$$
- $$t = 0$$ : absence of cancer, class $$C_2$$

이때 일반적인 classification 문제는 정상 환자의 X-ray를 $$C_2$$로, 암환자의 X-ray를 $$C_1$$로 분류하는 것이 된다.

#### Bayes' theorem in Disgnosis problem

진단 문제의 목표는 새로운 X-ray 사진 $$x$$가 있을 때 암세포가 있는지 없는지 정확하게 구분하는 것이다. 통계적으로는 $$P(C_k \lvert x)$$로 표현된다. 이와 같은 조건부 확률은 베이즈 정리에 따라 다음과 같은 등식이 성립한다.

$$
P(C_k \lvert x) = {P(x \lvert C_k)P(C_k) \over P(x)}
$$

#### Prior & Posterior

**사전 확률**(prior)이란 어떤 사건이 발생하기 이전의 확률을 말하고, **사후 확률**(posterior)은 반대로 어떤 사건이 발생한 후의 확률을 말한다. 위의 식에서 $$P(C_k)$$는 클래스 $$C_k$$에 대한 사전 확률 분포이고, $$P(C_k \lvert x)$$는 사후 확률 분포가 된다. 즉 X-ray 사진을 보기 전에 암환자의 사진일 확률은 $$P(C_1)$$으로 표현할 수 있고, 사진을 보고나서 그것이 암환자의 것일 확률은 $$P(C_1 \lvert x)$$라고 할 수 있다.

### Decision boundary

분류 문제를 확률 변수 $$X$$의 가능한 값 각각에 대해 정확한 클래스를 찾는 것으로 본다면, 전체 확률 변수 영역 내에서 어떤 영역에 존재하는 샘플에 대해서는 클래스 1로 분류하고, 그렇지 않은 경우에는 클래스 2로 분류하는 것으로 볼 수 있다. 이때 두 영역 간의 경계, 즉 안쪽에 포함되면 클래스 1이 되고 바깥쪽에 포함되면 클래스 2가 되는 경계를 **결정 경계**(decision boundary)라고 한다.

결정 경계는 새로운 샘플이 들어왔을 때 어떤 클래스로 분류할지 결정하는 기준이 된다. 그리고 분류 문제를 해결하기 위해 모델을 학습한다는 것은 결정 경계를 조정해나가는 것이 된다.

#### 결정 경계와 분류의 정확성

클래스 $$C_1$$로 분류되는 영역을 $$R_1$$, $$C_2$$로 분류되는 영역을 $$R_2$$라고 한다면 정확히 분류할 확률은 다음과 같이 표현이 가능하다.

$$
\eqalign{
P(correct) &= \Sigma_{k+1}^K P(X \in R_k, C_k)\\
&= \Sigma_{k=1}^K \int_{R_k} P(X, C_k) dX
}
$$

이때 $$P(X, C_k) = P(C_k \lvert X)P(X)$$가 성립하고, P(X)는 데이터의 분포이기 때문에 클래스에 따라 변화하지 않는다. 이러한 점에서 각 $$x$$를 사후분포 $$P(C_k \lvert x)$$가 가장 큰 $$C_k$$로 분류하면 정확하게 분류할 확률이 커진다고 할 수 있다.

반면 잘못 분류할 확률을 다음과 같이 표현할 수 있으며, 우리의 목표는 이것이 최소화되는 것이다.

$$
\eqalign{
P(mistake) &= P(X \in R_1, C_2) + P(X \in R_2, C_1) \\
&= \int_{R_1} P(X, C_2) dX + \int_{R_2} P(X, C_1) dX
}
$$

오분류의 가능성은 아래와 같이 그림으로 표현할 수도 있다.

<img src="{{site.image_url}}/study/decision_boundary_1.png" style="width: 30em">

위의 그림에서 $$P(x, C_1)$$은 $$x$$가 $$C_1$$일 확률을, $$P(x, C_2)$$는 $$C_2$$일 확률을 뜻한다. 그리고 $$\hat x$$는 현재의 결정 경계로, 왼쪽은 $$C_1$$로, 오른쪽은 $$C_2$$로 분류되는 영역이 된다. 그런데 위의 그림과 같이 결정 경계를 설정하면 잘못 분류하는 경우가 생기게 된다. 구체적으로는 빨간색과 초록색의 크기의 합만큼 원래 $$C_2$$인 것을 $$C_1$$으로 잘못 분류하게 되고 반대로 파란색의 크기만큼 $$C_1$$를 $$C_2$$로 분류하게 된다.

- $$C_1$$을 $$C_2$$로 오분류 : 파란색
- $$C_2$$을 $$C_1$$로 오분류 : 빨간색 + 초록색

#### 최적의 결정 경계

<img src="{{site.image_url}}/study/decision_boundary_2.png" style="width: 30em">

이러한 점을 고려할 때 최적의(optimal) 결정 경계는 위의 그림과 같이 두 확률 함수 $$P(x, C_1)$$, $$P(x, C_2)$$가 교차하는 지점이 된다. 이 경우 파란색과 초록색의 크기 만큼은 여전히 오분류가 발생하게 되지만 빨간색 영역 만큼은 줄어들게 된다.

- $$C_1$$을 $$C_2$$로 오분류 : 파란색
- $$C_2$$을 $$C_1$$로 오분류 : 초록색

### Loss function

**손실 함수**(loss function) 또는 비용 함수(cost function)는 잘못 분류한 경우 발생하는 손실과 관련된 함수이다. 즉 $$C_1$$을 $$C_2$$로 분류했을 때보다 $$C_2$$를 $$C_1$$으로 분류하는 것이 보다 심각한 문제를 초래하는 경우가 있는데, 이러한 점을 고려하여 오분류의 절대적인 크기가 아닌 오분류로 인한 손실의 크기를 표현하는 것이 손실 함수이다. 그리고 이러한 손실 함수의 크기를 최소화하는 것이 목표가 된다.

정답 클래스가 $$C_k$$, 모델에 의해 선택된 클래스가 $$C_j$$인 어떤 샘플 $$x$$가 있다고 하자. 그리고 잘못 선택하였을 때 발생하는 손실을 $$L_{kj}$$라고 하면, 전체 손실함수의 기대값은 다음과 같이 표현할 수 있다.

$$
E[L] = \Sigma_k \Sigma_j \int_{R_j} L_{kj}P(X, C_k) dX
$$

각각의 샘플 $$x$$는 어떤 클래스 $$C_j$$로 분류되는데, 매 경우 손실의 크기가 최소가 되도록 해야 한다. 위의 식을 고려할 때 각각의 $$x$$에 있어 $$\Sigma_k L_{kj}P(x, C_k)$$가 가장 작은 클래스 $$C_j$$를 선택하면 손실의 크기를 최소화 할 수 있다.

이때 사전 확률 $$P(X)$$가 클래스에 따라 바뀌지 않는 공통 인자임을 감안하면 다음 식을 최소화하는 $$C_j$$를 선택하는 문제가 된다.

$$
\Sigma_k L_{kj}P(C_k \lvert X)
$$
