---
layout: post
title: Bayesian Rule
category_num: 5
---

# Bayesian Rule

- update date : 2020.02.20

## 베이즈 정리

베이즈 정리는 prior, likelihood, posterior 간의 관계를 표현하는 식으로, 아래와 같다.

$$
P(A \lvert B) = {P(B \lvert A) P(A) \over P(B)}
$$

여기서 $$P(A)$$를 **prior**, $$P(B \lvert A)$$를 **likelihood** 그리고 $$P(A \lvert B)$$를 **posterior**라고 한다.

## 사전 확률, 가능도, 사후 확률

Prior, Likelihood, Posterior는 베이지언 확률에서 가장 기초가 되는 개념으로, 우리말로 사전 확률, 가능도(우도), 사후 확률로 불린다.

- **Prior**: 원인 사건이 발생할 확률
- **Likelihood**: 원인 사건이 발생했을 때 결과 사건이 발생할 확률
- **Posterior**: 결과 사건이 발생했을 때 원인 사건이 발생했을 확률

위와 같은 정의만 보아서는 무엇을 의미하는지 추상적으로만 들린다. 다음과 같은 구체적인 예시를 생각하면 이해하는 데에 도움이 된다.

### 사과 박스에 돈 뭉치

어떤 박스가 있을 때 사과라고 쓰여 있으면 $$A=a_1$$, 돈이라고 쓰여 있으면 $$A=a_2$$라고 하자. 그리고 어떤 박스에 사과가 들어 있으면 $$B=b_1$$, 돈이 들어 있으면 $$B=b_2$$라고 하자. 그렇다면 아래 각각의 확률 함수의 의미는 다음과 같다.

- $$P(A = a_1)$$ : 어떤 박스에 사과가 들어 있을 확률
- $$P(B = b_1)$$ : 어떤 박스에 사과라고 쓰여 있을 확률
- $$P(A = a_1, B = b_1)$$ : 어떤 박스에 사과라고 쓰여 있으면서 사과가 들어 있을 확률

조건부 확률의 의미를 고려하면 $$P(A = a_1 \lvert B = b_1)$$는 다음과 같다.

- $$P(B = b_1 \lvert A = a_1)$$ : 사과가 들어 있을 때 박스에 사과라고 쓰여 있을 확률

이때 조건부 확률은 그 정의에 따라 다음과 같이 쓸 수 있다.

$$
P(X|Y) = {P(X, Y) \over P(Y) }
$$

이를 적용하면 $$P(B = b_1 \lvert A = a_1)$$는 다음과 같이 풀어 쓸 수 있다.

$$
\eqalign{
P(B = b_1 \lvert A = a_1)
&= {P(A = a_1, B = b_1) \over P(A = a_1)} \\
&= {P(A = a_1 \lvert B = b_1) P(B = b_1) \over P(A = a_1)}
}
$$

이를 prior, likelohood, posterior로 분리해 그 의미를 생각해보면 다음과 같다.

- **prior**: 어떤 박스에 사과라고 쓰여 있을 확률
- **likelihood**: 사과라고 쓰여 있는 박스를 열었을 때 사과가 들어 있을 확률
- **posterior**: 사과가 들어 있는 박스를 보니 사과라고 쓰여 있을 확률

## 머신러닝과 베이즈 정리

예시를 아래와 같이 바꾸어보자. 이제 우리의 목표는 돈이 들어 있는 박스를 찾는 것이다.

- **prior**: 스캐너가 돈다발이 들어 있다고 판단할 확률
- **likelihood**: 돈다발로 판단된 박스를 열었을 때 돈다발이 들어 있을 확률
- **posterior**: 돈다발이 들어 있는 박스를 보니 돈다발로 판단되었을 확률

이전 예시와 비교해 바뀐 점이 있다면 스캐너가 새롭게 추가되었다는 것이다.

그런데 문제가 있다면 스캐너가 완벽하지 못해 돈다발이라고 판단한 것 중에 사과가 들어있는 경우가 많이 있다는 것이다. 이러한 문제를 해결하기 위해 스캐너를 업데이트 해야하는데, 가장 쉽게 생각할 수 있는 방법이 돈다발로 판단한 모든 박스를 열어보고 그것을 토대로 반복된 실수를 하지 않도록 하는 것이다. 즉, 경험을 토대로 학습하는 것이다.

### 경험을 통한 학습

만약 박스가 $$X = {x_1, x_2, ... x_{10}}$$로 총 10개 있고, 그 중 실제로 돈다발이 들어 있는 박스는 $$x_1, x_2, x_9$$라고 하자. 이때 스캐너가 전체 박스 중 $$x_2, x_5$$에 돈다발이 들어 있다고 판단했다면, **prior**는 $$P(alarm) = 0.2$$가 된다. 그런데 돈다발이 들어 있다고 판단된 박스 두 개를 열어보니 $$x_2$$에는 돈다발이 있었지만, $$x_5$$에는 사과가 있었다. 즉, **likelihood** $$P(money \lvert alarm)$$는 0.5이다.

prior와 likelihood를 모두 구했지만 posterior를 구하기 위해서는 분모 $$p(money)$$를 구해야 한다.

$$
P(money) = P(money \lvert alarm) P(alarm) + P(money \lvert \backsim alarm) P(\backsim alarm)
$$

위 식을 계산하면 $$P(money) = 0.3$$이 되는 것을 알 수 있다.

$$
{P(money \lvert alarm) P(alarm) \over P(money)} = { 0.5 * 0.2 \over 0.3} = {1 \over 3}
$$

결과적으로 **posterior** $$P(alarm \lvert money)$$는 $$1 \over 3$$으로 구해지며, 이를 또다시 동일한 박스 10개를 테스트할 때 prior로 사용하는 것을 학습이라고 할 수 있다. 실제 optimal한 prior가 0.3임을 감안하면 학습의 결과로 optimal 값에 보다 가까워진 것이라고 할 수 있다.

조금 어렵게 말한 감이 있지만 결국에는 다음 예측을 할 때에는 $$alarm$$이 울렸을 때 $$money$$가 들어 있었는지 아닌지 확인해보고 결정한 $$P(alarm \lvert money)$$을 토대로 $$P(alarm)$$를 고려하면 보다 정확하다는, 다소 상식적인 이야기가 된다.
