---
layout: post
title: 3. Norms
category_num : 3
---

# Norms

- Ian Goodfellow, Yoshua Bengio, Aaron Courville의 Deep Learning Book을 참고하여 작성했습니다. 따라서 선형대수를 깊게 다루지는 않고 머신러닝에 있어 중요한 내용들로 이뤄져 있습니다.
- update at : 20.01.25

## Norm

Norm이란 벡터의 크기를 측정하기 위해 사용하는 함수를 말한다. $$L_p$$ norm은 다음과 같다.

$$
\| x \|_p = (\Sigma_i \lvert x_i \rvert^p)^{1 \over p}, \qquad \text{for} \ p \in \rm l\!R, \ p \geqq 1
$$

크기를 측정하는 함수로서 $$L_p$$ norm은 벡터를 입력으로 받아 음수가 아닌 실수의 값을 반환한다. 직관적으로는 origin부터 벡터 $$x$$가 가리키는 지점 사이의 거리를 의미하며, 다음과 같은 특성을 만족하는 함수이다.

- $$f(x) = 0 \rightarrow x = 0$$
- $$f(x+y) \geqq f(x) + f(y)$$
- $$\text{for all} \ \alpha \in \rm l\!R, \ f(\alpha x) = \lvert \alpha \rvert f(x)$$

### $$L^2$$ norm

가장 많이 사용되는 $$L_p$$ norm은 $$p=2$$인 $$L^2$$ norm이다. $$L^2$$ norm은 origin에서 벡터 x까지의 거리가 유클리디안 거리로 측정한 것과 동일하기 때문에 Euclidean norm으로도 불린다. 머신러닝에서 자주 사용되기 때문에 $$\| x \|$$와 같이 2를 생략하고 표기하기도 한다.

$$L^2$$ norm과 함께 squared $$L^2$$ norm도 많이 사용된다. squared $$L^2$$ norm은 $$x^T x$$로 비교적 단순하게 계산할 수 있으므로 편리하다. 또한 squared $$L^2$$ norm의 미분값은 전체 벡터에 영향을 받는 $$L^2$$ norm과는 달리 관련있는 요소에 대해서만 영향을 받는다는 점에서도 이점이 있다.

### $$L^1$$ norm

하지만 squared $$L^2$$ norm은 origin 부근에서는 매우 천천히 증가한다는 단점이 있다. 몇몇 머신러닝 방법론 중에는 0일 때와 그렇지 않을 때를 엄밀하게 구분할 것을 요구하기도 하는데 이러한 경우에는 squared $$L^2$$ norm이 적절하지 않다고 할 수 있다.

$$L^1$$ norm은 계산의 편의성을 유지하면서도 이러한 문제를 해소할 수 있는 대안으로 사용된다.

$$
\| x \|_1 = \Sigma_i \lvert x_i \rvert
$$

### $$L^{\infty}$$ norm

$$L^{\infty}$$ norm은 max norm이라고도 불리는데, 다음과 같이 전체 element 중 크기가 가장 큰 것으로 계산된다.

$$
\| x \|_{\infty} = max_i \lvert x_i \rvert
$$

### $$L^2$$ norm과 dot product

어떤 두 벡터 간의 dot product는 다음과 같이 $$L^2$$ Norm의 곱으로 표현할 수 있다.

$$
x^T y = \| x \|_2 \| y \|_2 \cos \theta, \qquad \theta \ \text{is the angle between x and y}
$$
